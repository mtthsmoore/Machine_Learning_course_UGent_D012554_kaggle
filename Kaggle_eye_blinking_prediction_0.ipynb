{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Kaggle_eye_blinking_prediciton_0.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mtthsmoore/Machine_Learning_course_UGent_D012554_kaggle/blob/master/Kaggle_eye_blinking_prediction_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dX2xmvzf0yH",
        "colab_type": "text"
      },
      "source": [
        "Use scikit 10-fold cross-validation to compute estimate of generalization performance of the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJijsWJyfvfz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import cross_val_predict\n",
        "\n",
        "cv_predictions = cross_val_predict(model,X,y,cv=5)\n",
        "print(cv_predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hYRcvF4fVyn",
        "colab_type": "text"
      },
      "source": [
        "To optimize the degree $d$ we can now use the CV loop:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4FV9R_eBmif",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = dataset[dataset['folds']==0].copy()\n",
        "y = X.pop('y')\n",
        "folds = X.pop('folds')\n",
        "\n",
        "scores = []\n",
        "scores.append(0) # no features (d=0)\n",
        "cv_predictions = cross_val_predict(model, X, y, cv=5) # (d=1)\n",
        "scores.append(metrics.r2_score(y,cv_predictions)) \n",
        "for degree in range(2,18,1):\n",
        "    X['x1^'+str(degree)] = X['x1']**degree\n",
        "    X['x1^'+str(degree)] = feature_scaler.fit_transform(X[ ['x1^'+str(degree)]])\n",
        "    cv_predictions = cross_val_predict(model, X, y, cv=5) # (d=1)\n",
        "    scores.append(metrics.r2_score(y,cv_predictions)) \n",
        "\n",
        "tmp = pd.DataFrame(scores,columns=['cv_score'])\n",
        "tmp.plot(ylim=(0,1))\n",
        "plt.show()\n",
        "print(\"best CV score: {}\".format(max(scores)))\n",
        "print(\"optimal degree d: {}\".format(np.argmax(scores)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRt0mQS4g2ML",
        "colab_type": "text"
      },
      "source": [
        "Should we consider this best CV score a good estimated of the generalization performance of a model with optimal degree $d$? No. The CV predictions were used to make the decision on the optimal degree $d$. Instead we need a second CV loop that is nested in the first loop to estimate the generalization performance while minimizing the possibility of overfitting. This is called **nested-CV** and will be explained further in this course."
      ]
    }
  ]
}